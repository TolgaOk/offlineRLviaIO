{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "env = gym.make(\"walker2d-medium-v2\")\n",
    "dataset = env.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"next_observations\"].shape\n",
    "\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.env_checker import PassiveEnvChecker\n",
    "from gymnasium.wrappers.order_enforcing import OrderEnforcing\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "from gymnasium.envs.mujoco.walker2d_v4 import Walker2dEnv\n",
    "\n",
    "\n",
    "official_env = gym.make(\"Walker2d-v4\")\n",
    "unofficial_env = TimeLimit(OrderEnforcing(PassiveEnvChecker(\n",
    "    Walker2dEnv(exclude_current_positions_from_observation=False)\n",
    ")), max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = unofficial_env.reset()\n",
    "action = np.zeros_like(unofficial_env.action_space.low)\n",
    "next_state, reward, term, trun, info = unofficial_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(- np.sum(np.square(action)) * unofficial_env.env.env.env._ctrl_cost_weight\n",
    " + 1.0\n",
    " + (next_state[0] - state[0]) / unofficial_env.env.env.env.dt * unofficial_env.env.env.env._forward_reward_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PPO agent to collect dataset\n",
    "\n",
    "- Train PPO agent\n",
    "- Use \"half\" trained PPO agent and collect a dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "import os\n",
    "import multiprocessing\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from io_agent.control.ppo import PPoController\n",
    "\n",
    "from common import run_mpc, run_io_mpc\n",
    "from utils import parallelize, steady_state_cost, save_experiment\n",
    "\n",
    "\n",
    "general_seed = 42\n",
    "ppo_path = f\"/mnt/DEPO/tok/sl-to-rl/walker/ppo-3m-{general_seed}\"\n",
    "\n",
    "if not os.path.exists(\".\".join([ppo_path, \"zip\"])):\n",
    "    PPoController.train(lambda: gym.make(\"Walker2d-v4\"),\n",
    "                        n_envs=16,\n",
    "                        seed=general_seed,\n",
    "                        path=ppo_path,\n",
    "                        total_timesteps=int(3e6),\n",
    "                        ppo_kwargs=dict(\n",
    "                            ent_coef=1e-4,\n",
    "                            learning_rate=3e-4),\n",
    "                        )\n",
    "ppo_agent = PPoController(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Any, Callable, Tuple, Type\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "from io_agent.plant.base import NominalLinearEnvParams, LinearConstraint, LinearConstraints\n",
    "from io_agent.evaluator import ControlLoop, Transition\n",
    "from io_agent.control.mpc import MPC\n",
    "from io_agent.control.rmpc import RobustMPC\n",
    "from io_agent.control.io import IOController, AugmentedTransition, AugmentDataset\n",
    "from io_agent.utils import FeatureHandler\n",
    "\n",
    "\n",
    "env = gym.make(\"walker2d-medium-v2\")\n",
    "dataset = env.get_dataset()\n",
    "\n",
    "# nominal_model = NominalLinearEnvParams(\n",
    "#     matrices=None,\n",
    "#     constraints=LinearConstraints(\n",
    "#         action=LinearConstraint(\n",
    "#             matrix=np.block([\n",
    "#                 [np.eye(env.action_space.shape[0])],\n",
    "#                 [-np.eye(env.action_space.shape[0])]]),\n",
    "#             vector=np.concatenate([\n",
    "#                 env.action_space.high,\n",
    "#                 -env.action_space.low\n",
    "#             ])\n",
    "#         )\n",
    "#     ),\n",
    "#     costs=None\n",
    "# )\n",
    "\n",
    "# feature_handler = FeatureHandler(\n",
    "#     params=nominal_model,\n",
    "#     n_past=0,\n",
    "#     add_bias=False,\n",
    "#     use_action_regressor=False,\n",
    "#     use_noise_regressor=False,\n",
    "#     use_state_regressor=False,\n",
    "#     noise_size=0,\n",
    "#     state_size=env.observation_space.shape[0],\n",
    "#     action_size=env.action_space.shape[0],\n",
    "#     output_size=env.observation_space.shape[0],\n",
    "# )\n",
    "# augmenter = AugmentDataset(\n",
    "#     expert_agent=None,\n",
    "#     feature_handler=feature_handler\n",
    "# )\n",
    "\n",
    "\n",
    "# general_seed = 42\n",
    "# rng = np.random.default_rng(general_seed)\n",
    "# indices = rng.integer(0, len(dataset[\"observations\"]), size = 1000)\n",
    "\n",
    "# trajectory = [Transition(\n",
    "#     state=dataset[\"observations\"][index],\n",
    "#     next_state=dataset[\"next_observations\"][index],\n",
    "#     action=dataset[\"actions\"][index],\n",
    "#     reward=dataset[\"observation\"][index],\n",
    "#     state=dataset[\"observation\"][index],\n",
    "#     state=dataset[\"observation\"][index],\n",
    "# ) for index in indices]\n",
    "# augmenter([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## Iterative IO algorithm\n",
    "- - -\n",
    "#### Quadrotor Environment\n",
    "\n",
    "The Q minimization:\n",
    "$$\n",
    "\\min_u 2s^T\\theta_{su}u + u^T\\theta_{uu}u \\\\ \\text{s.t.} \\quad G_u u \\leq h_u\n",
    "$$\n",
    "\n",
    "- Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from io_agent.evaluator import Transition\n",
    "from io_agent.plant.quadrotor import QuadrotorEnv\n",
    "\n",
    "from common import run_mpc, run_io_rmpc\n",
    "from utils import parallelize, steady_state_cost, save_experiment\n",
    "\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "horizon = 25\n",
    "n_trials = 20\n",
    "n_past = 2\n",
    "add_bias = False\n",
    "n_dataset_trials = 20\n",
    "n_rhos = 12\n",
    "general_seed = 42\n",
    "seed_rng = np.random.default_rng(general_seed)\n",
    "\n",
    "plant = QuadrotorEnv()\n",
    "permute_seed, *trial_seeds = seed_rng.integers(0, 2**30, n_trials + 1)\n",
    "dataset_trial_seeds = seed_rng.integers(0, 2**30, n_dataset_trials)\n",
    "\n",
    "dataset_trajectories = parallelize(\n",
    "    n_proc=min(n_cpu, n_dataset_trials),\n",
    "    fn=partial(run_mpc, plant=plant),\n",
    "    kwargs_list=[\n",
    "        dict(\n",
    "            horizon=horizon,\n",
    "            use_foresight=False,  # Without hindsight data\n",
    "            bias_aware=False,\n",
    "            env_reset_rng=np.random.default_rng(_seed)\n",
    "        ) for _seed in dataset_trial_seeds\n",
    "    ],\n",
    "    loading_bar_kwargs=dict(desc=\"MPC dataset trials\")\n",
    ")\n",
    "\n",
    "save_experiment(\n",
    "    values={\"mpc_dataset\": dataset_trajectories},\n",
    "    seed=general_seed,\n",
    "    exp_dir=\"./quadrotor_data/dataset\",\n",
    "    name=\"mpc_obl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct the Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple, Optional, Any, Union, Dict\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import geotorch\n",
    "import cvxpy as cp\n",
    "\n",
    "from io_agent.plant.quadrotor import QuadrotorEnv\n",
    "from io_agent.plant.base import (NominalLinearEnvParams,\n",
    "                                 LinearizationWrapper,\n",
    "                                 LinearConstraints,\n",
    "                                 LinearConstraint,\n",
    "                                 Plant)\n",
    "from io_agent.control.mpc import Optimizer\n",
    "from io_agent.utils import AugmentedTransition, FeatureHandler\n",
    "from io_agent.control.io import AugmentDataset\n",
    "from io_agent.control.mpc import MPC\n",
    "from io_agent.control.deep_io import IterativeIOController\n",
    "from io_agent.evaluator import Transition\n",
    "\n",
    "from utils import load_experiment, parallelize\n",
    "from common import run_agent\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DeepIOArgs():\n",
    "    n_past: int = 2\n",
    "    add_bias: int = False\n",
    "    use_action_regressor: bool = False\n",
    "    use_noise_regressor: bool = True\n",
    "    use_state_regressor: bool = False\n",
    "    horizon: int = 25\n",
    "    use_expert: bool = False\n",
    "    learning_rate: float = 1e-2\n",
    "    lr_exp_decay: float = 0.98\n",
    "    n_epoch: int = 1000\n",
    "    n_batch: int = 128\n",
    "\n",
    "\n",
    "def prepare_deep_io(dataset: List[List[Transition]],\n",
    "                  env: Plant,\n",
    "                  rng: np.random.Generator,\n",
    "                  args: DeepIOArgs,\n",
    "                  verbose: bool = True\n",
    "                  ) -> IterativeIOController:\n",
    "    feature_handler = FeatureHandler(\n",
    "        params=env.nominal_model(),\n",
    "        n_past=args.n_past,\n",
    "        add_bias=args.add_bias,\n",
    "        use_action_regressor=args.use_action_regressor,\n",
    "        use_noise_regressor=args.use_noise_regressor,\n",
    "        use_state_regressor=args.use_state_regressor,\n",
    "    )\n",
    "    expert_agent = None\n",
    "    if args.use_expert:\n",
    "        expert_agent = MPC(\n",
    "            action_size=lin_env.action_size,\n",
    "            state_size=lin_env.state_size,\n",
    "            noise_size=lin_env.noise_size,\n",
    "            output_size=lin_env.output_size,\n",
    "            horizon=args.horizon)\n",
    "        expert_agent.optimizer = expert_agent.prepare_optimizer(feature_handler.params)\n",
    "    augmenter = AugmentDataset(\n",
    "        expert_agent=expert_agent,\n",
    "        feature_handler=feature_handler,\n",
    "    )\n",
    "    augmented_dataset = augmenter(dataset)\n",
    "    torch.manual_seed(rng.integers(0, 2**30).item())\n",
    "    iterative_io_agent = IterativeIOController(\n",
    "        constraints=feature_handler.params.constraints,\n",
    "        feature_handler=feature_handler,\n",
    "        learning_rate=args.learning_rate,\n",
    "        include_constraints=True,\n",
    "        action_constraints_flag=True,\n",
    "        state_constraints_flag=False,\n",
    "        lr_exp_decay=args.lr_exp_decay,\n",
    "    )\n",
    "    return iterative_io_agent, augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/DEPO/tok/miniconda3/lib/python3.10/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ec2ea4f9f14ba2a247ab313dbdec70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_break_epoch 10\n",
      "(array([0.11871567, 0.13893078], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from common import run_agent\n",
    "from utils import save_experiment\n",
    "\n",
    "\n",
    "lin_env = LinearizationWrapper(QuadrotorEnv())\n",
    "dataset = load_experiment(\"./quadrotor_data/dataset/mpc_obl-42\")[\"mpc_dataset\"]\n",
    "\n",
    "n_trials = 50\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "general_seed = 43\n",
    "seed_rng = np.random.default_rng(general_seed)\n",
    "trial_seeds = seed_rng.integers(0, 2**30, n_trials)\n",
    "evaluation_epochs = [int(val) for val in np.floor(np.logspace(1, 2, 15))]\n",
    "\n",
    "\n",
    "def deep_io_trials(args: DeepIOArgs, rng: np.random.Generator, trial_seeds: List[int]):\n",
    "    if args.n_epoch != 0:\n",
    "        raise ValueError(\"```n_epoch``` must be set to 0\")\n",
    "    deep_io_agent, augmented_dataset = prepare_deep_io(\n",
    "        dataset, lin_env, rng, args, verbose=False)\n",
    "    losses = []\n",
    "    costs = {}\n",
    "    with tqdm(total=evaluation_epochs[-1]) as pbar:\n",
    "        for eval_break_epoch in evaluation_epochs:\n",
    "            print(\"eval_break_epoch\", eval_break_epoch)\n",
    "            n_epoch = eval_break_epoch - len(losses)\n",
    "            _losses = deep_io_agent.train(\n",
    "                augmented_dataset,\n",
    "                epochs=n_epoch,\n",
    "                batch_size=args.n_batch,\n",
    "                rng=rng,\n",
    "                verbose=False)\n",
    "            losses.extend(_losses)\n",
    "\n",
    "            deep_io_trajectories = parallelize(\n",
    "                n_proc=min(n_cpu, 50),\n",
    "                fn=partial(run_agent, agent=deep_io_agent, plant=lin_env),\n",
    "                kwargs_list=[\n",
    "                    dict(\n",
    "                        use_foresight=False,\n",
    "                        bias_aware=False,\n",
    "                        env_reset_rng=np.random.default_rng(_seed)\n",
    "                    ) for _seed in trial_seeds\n",
    "                ],\n",
    "            )\n",
    "            deep_io_costs = [300 - np.sum([np.exp(-tran.cost) for tran in trajectory])\n",
    "                             for trajectory in deep_io_trajectories]\n",
    "            costs[eval_break_epoch] = deep_io_costs\n",
    "            pbar.set_postfix({\"Median cost\": np.median(deep_io_costs)})\n",
    "            pbar.update(n_epoch)\n",
    "    return costs, losses\n",
    "\n",
    "\n",
    "experiment_args = {\n",
    "    \"DeepIO-No-expert\": DeepIOArgs(use_expert=False, n_epoch=0),\n",
    "    # \"DeepIO-MPC-expert\": DeepIOArgs(use_expert=True, n_epoch=0, lr_exp_decay=0.97, n_batch=32),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for key, args in experiment_args.items():\n",
    "    results[key] = deep_io_trials(\n",
    "        args,\n",
    "        np.random.default_rng(seed_rng.integers(0, 2*30)),\n",
    "        trial_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_experiment(\n",
    "    values={\"deep-io\": results},\n",
    "    seed=general_seed,\n",
    "    exp_dir=\"./quadrotor_data/ablation\",\n",
    "    name=\"deep-io_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.ticker as tck\n",
    "\n",
    "from utils import steady_state_cost, load_experiment\n",
    "from collections import defaultdict\n",
    "from plotter import histogram_figure, histogram_figure_plt, tube_figure_plt\n",
    "\n",
    "\n",
    "fig = histogram_figure_plt(\n",
    "    cost_data={key: value[0][100] for key, value in results.items()},\n",
    "    title=\"\",\n",
    "    bw_method=\"scott\",\n",
    "    bw_adjust=0.30,\n",
    "    log_yaxis=True,\n",
    "    y_label=\"log density\",\n",
    "    low_y=1e-3,\n",
    "    figsize=(6, 3)\n",
    ")\n",
    "fig.savefig(f\"figure_data/hist_deep.svg\", format=\"svg\", dpi=1200)\n",
    "\n",
    "fig, axes = tube_figure_plt(\n",
    "    cost_data={key: value[0] for key, value in results.items()},\n",
    "    title=f\"\",\n",
    "    log_xaxis=True,\n",
    "    log_yaxis=False,\n",
    "    x_label=\"epoch\",\n",
    "    y_label=\"episodic cost\",\n",
    "    percentiles=(20, 80)\n",
    ")\n",
    "fig.savefig(f\"figure_data/perf_vs_epoch_deep.svg\", format=\"svg\", dpi=1200)\n",
    "\n",
    "fig, axes = tube_figure_plt(\n",
    "    cost_data={key: {index + 1: value for index, value in enumerate(value[1])} for key, value in results.items()},\n",
    "    title=f\"\",\n",
    "    log_xaxis=True,\n",
    "    log_yaxis=True,\n",
    "    x_label=\"epoch\",\n",
    "    y_label=\"sub loss\",\n",
    "    percentiles=(20, 80)\n",
    ")\n",
    "fig.savefig(f\"figure_data/loss_vs_epoch_deep.svg\", format=\"svg\", dpi=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{key: {_key: np.median(_val) for _key, _val in value[0].items()} for key, value in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from io_agent.control.io import IOController\n",
    "\n",
    "\n",
    "# io_agent = IOController(\n",
    "#     params=nominal_dyn,\n",
    "#     dataset_length=300,\n",
    "#     feature_handler=feature_handler,\n",
    "#     include_constraints=True,\n",
    "#     action_constraints_flag=True,\n",
    "#     state_constraints_flag=False,\n",
    "# )\n",
    "# io_agent.train(\n",
    "#     augmented_dataset,\n",
    "#     rng=np.random.default_rng(42))\n",
    "# io_agent.action_optimizer = io_agent.prepare_action_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TEST] Is minimizer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = np.random.default_rng(42).permutation(len(dataset))[:10]\n",
    "aug_states = [augmented_dataset[index].aug_state for index in batch_indices]\n",
    "exp_actions = [augmented_dataset[index].action for index in batch_indices]\n",
    "\n",
    "actions = iterative_io_agent.batch_minimizer_actions(\n",
    "    aug_states=torch.from_numpy(np.stack(aug_states, axis=0)).float(),\n",
    "    theta_uu=iterative_io_agent.th_theta_uu.weight,\n",
    "    theta_su=iterative_io_agent.th_theta_su.weight\n",
    ").detach().cpu().numpy()\n",
    "\n",
    "index = -1\n",
    "theta_uu = iterative_io_agent.th_theta_uu.weight.detach().cpu().numpy()\n",
    "theta_su = iterative_io_agent.th_theta_su.weight.detach().cpu().numpy()\n",
    "\n",
    "def q_function(act, state):\n",
    "    return (act * (theta_uu @ act)).sum() + 2 * (state * (theta_su @ act)).sum()\n",
    "\n",
    "print(\"Solution:\", q_function(actions[index], aug_states[index]))\n",
    "\n",
    "values = [q_function(lin_env.action_space.sample(), aug_states[index]) for _ in range(100)]\n",
    "min(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TEST] Is loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = np.random.default_rng(42).permutation(len(augmented_dataset))[:30]\n",
    "aug_states = [augmented_dataset[index].aug_state for index in batch_indices]\n",
    "exp_actions = [augmented_dataset[index].action for index in batch_indices]\n",
    "\n",
    "th_aug_states = torch.from_numpy(np.stack(aug_states, axis=0)).float()\n",
    "th_exp_actions = torch.from_numpy(np.stack(exp_actions, axis=0)).float()\n",
    "\n",
    "th_min_actions = iterative_io_agent.batch_minimizer_actions(\n",
    "    aug_states=torch.from_numpy(np.stack(aug_states, axis=0)).float(),\n",
    "    theta_uu=iterative_io_agent.th_theta_uu.weight,\n",
    "    theta_su=iterative_io_agent.th_theta_su.weight\n",
    ")\n",
    "\n",
    "losses = iterative_io_agent.loss(th_aug_states, th_exp_actions)\n",
    "th_min_actions[0], exp_actions[0]\n",
    "\n",
    "\n",
    "exp_q = iterative_io_agent.q_fn(th_aug_states, th_exp_actions, iterative_io_agent.th_theta_uu.weight)\n",
    "exp_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_q = iterative_io_agent.q_fn(th_aug_states, th_min_actions, iterative_io_agent.th_theta_uu.weight)\n",
    "min_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[io_cost(aug_state, exp_action) for aug_state, exp_action in zip(aug_states, exp_actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_q - min_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_io_agent.th_theta_uu.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_agent.action_optimizer = io_agent.prepare_action_optimizer()\n",
    "io_agent.reset()\n",
    "io_agent.action_optimizer\n",
    "\n",
    "\n",
    "def io_actions(aug_state):\n",
    "    constraint_matrix, constraint_vector = io_agent.calculate_constraints(\n",
    "        io_agent.feature_handler.original_state(aug_state))\n",
    "    io_agent.action_optimizer.parameters[\"constraint_matrix\"].value = constraint_matrix\n",
    "    io_agent.action_optimizer.parameters[\"constraint_vector\"].value = constraint_vector\n",
    "    io_agent.action_optimizer.parameters[\"state\"].value = aug_state\n",
    "\n",
    "    solution = io_agent.action_optimizer.problem.solve()\n",
    "    return io_agent.action_optimizer.variables[\"action\"].value\n",
    "\n",
    "def io_function(aug_state, action):\n",
    "    return 2 * np.sum(aug_state * (io_agent._q_theta_su @ action)) + (action * (io_agent._q_theta_uu @ action)).sum()\n",
    "\n",
    "def io_cost(aug_state, exp_action):\n",
    "    min_action = io_actions(aug_state)\n",
    "    return io_function(aug_state, exp_action) - io_function(aug_state, min_action)\n",
    "\n",
    "io_cost(aug_states[10], exp_actions[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_agent.evaluator import ControlLoop\n",
    "\n",
    "deep_io_agent.action_optimizer = deep_io_agent.prepare_action_optimizer()\n",
    "evaluator = ControlLoop(plant=lin_env, controller=deep_io_agent, rng=np.random.default_rng(42))\n",
    "trajectory = evaluator.simulate(\n",
    "    bias_aware=False,\n",
    "    use_foresight=False,\n",
    ")\n",
    "\n",
    "np.sum([np.exp(-t.cost) for t in trajectory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "states = np.stack([t.state for t in dataset[\"mpc_dataset\"][0]][:batch_size], axis=0)\n",
    "\n",
    "\n",
    "params[\"state\"].value = states\n",
    "params[\"theta_su\"].value = theta_su.detach().numpy()\n",
    "params[\"theta_uu\"].value = theta_uu.detach().numpy()\n",
    "solution = problem.solve()\n",
    "solution, params[\"action\"].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"action\"].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "random_actions = np.clip(np.random.randn(1, lin_env.action_size), lin_env.action_space.low.reshape(1, -1), lin_env.action_space.high.reshape(1, -1))\n",
    "\n",
    "np.min(2 * np.sum(states[:1, :] * (theta_su.detach().numpy() @ random_actions.T).T, axis=-1)\n",
    "+ np.sum(random_actions * (theta_uu.detach().numpy() @ random_actions.T).T, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_dyn.constraints.action.matrix, nominal_dyn.constraints.action.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## Convex Layers\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "\n",
    "out = cp.Parameter(shape=(10))\n",
    "ws_1 = [cp.Variable(shape=(10, 10)) for _ in range(1)]\n",
    "ws_2 = [cp.Variable(shape=(10, 10)) for _ in range(1)]\n",
    "ws_3 = [cp.Variable(shape=(10, 10)) for _ in range(1)]\n",
    "\n",
    "for ws in [ws_1, ws_2, ws_3]:\n",
    "    logits = cp.vstack([out @ w.T for w in ws])\n",
    "    out = logits\n",
    "    # out = cp.max(logits, axis=0)\n",
    "\n",
    "out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## Parametric Set of Linear Inequalities\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "\n",
    "\n",
    "g_matrix = np.array([\n",
    "    [0.75, 0.25],\n",
    "    [-0.25, 1],\n",
    "    [-0.75, 0.25]\n",
    "])\n",
    "\n",
    "h_vector = np.array([\n",
    "    0.2,\n",
    "    0.2,\n",
    "    0.2\n",
    "])\n",
    "\n",
    "\n",
    "def visualize(the_point: np.ndarray) -> None:\n",
    "    plt.figure(dpi=100, figsize=(5, 4))\n",
    "    x_points = np.linspace(-1.5, 1.5, 500).reshape(1, -1)\n",
    "    y_points_matrix = ((h_vector.reshape(-1, 1) -\n",
    "                       g_matrix[:, 0].reshape(-1, 1) * x_points) / g_matrix[:, 1].reshape(-1, 1))\n",
    "\n",
    "    for y_points in y_points_matrix:\n",
    "        plt.plot(x_points.ravel(), y_points, \"k--\")\n",
    "\n",
    "    x_grid, y_grid = np.meshgrid(x_points, x_points)\n",
    "    z_flat = np.all(\n",
    "        g_matrix @ np.stack([x_grid.ravel(), y_grid.ravel()], axis=0) <= h_vector.reshape(-1, 1),\n",
    "        axis=0)\n",
    "    plt.contourf(x_grid, y_grid, z_flat.astype(np.float64).reshape(\n",
    "        x_grid.shape), cmap=plt.cm.twilight, alpha=0.4)\n",
    "\n",
    "    plt.plot(0, 0, \"go\", markersize=5)\n",
    "    plt.plot(*the_point, \"ro\", markersize=5)\n",
    "\n",
    "    plt.xlim(-1.5, 1.5)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize(np.array([0.3, 1.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigh(g_matrix @ g_matrix.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
