{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fighter Environment\n",
    "\n",
    "Experiments and figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Union, Any, Callable, Tuple, Type\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Process, Queue\n",
    "from tqdm.notebook import tqdm\n",
    "import queue\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "from io_agent.plant.fighter import FighterEnv\n",
    "from io_agent.plant.base import Plant, LinearizationWrapper, InputValues\n",
    "from io_agent.evaluator import ControlLoop, Transition\n",
    "from io_agent.control.mpc import MPC\n",
    "from io_agent.control.rmpc import RobustMPC\n",
    "from io_agent.control.io import IOController, AugmentedTransition, AugmentDataset\n",
    "from io_agent.utils import FeatureHandler\n",
    "\n",
    "\n",
    "def parallelize(n_proc: int,\n",
    "                fn: Callable[[Any], Any],\n",
    "                kwargs_list: List[Dict[str, Any]],\n",
    "                loading_bar_kwargs: Optional[Dict[str, Any]] = None\n",
    "                ) -> List[Any]:\n",
    "\n",
    "    def _async_execute_wrapper() -> None:\n",
    "        while True:\n",
    "            try:\n",
    "                kwargs, key = work_queue.get(block=False)\n",
    "            except queue.Empty:\n",
    "                return None\n",
    "            result = fn(**kwargs)\n",
    "            result_queue.put({\"key\": key, \"result\": result})\n",
    "\n",
    "    result_queue = Queue()\n",
    "    work_queue = Queue()\n",
    "\n",
    "    for key, kwargs in enumerate(kwargs_list):\n",
    "        work_queue.put((kwargs, key))\n",
    "\n",
    "    loading_bar = (partial(tqdm, **loading_bar_kwargs)\n",
    "                   if loading_bar_kwargs is not None\n",
    "                   else lambda x: x)\n",
    "\n",
    "    process_list = []\n",
    "    for _ in range(n_proc):\n",
    "        process_list.append(Process(target=_async_execute_wrapper))\n",
    "        process_list[-1].start()\n",
    "\n",
    "    results_dict = {}\n",
    "    for _ in loading_bar(range(len(kwargs_list))):\n",
    "        _return = result_queue.get(block=True)\n",
    "        results_dict[_return[\"key\"]] = _return[\"result\"]\n",
    "\n",
    "    for process in (process_list):\n",
    "        process.join()\n",
    "\n",
    "    results = [results_dict[index] for index in range(len(results_dict))]\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_experiment(values: Any, seed: int, exp_dir: str, name: str) -> None:\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    with open(os.path.join(exp_dir, f\"{name}-{seed}\"), \"wb\") as fobj:\n",
    "        pickle.dump(values, fobj)\n",
    "\n",
    "\n",
    "def load_experiment(path: str) -> Any:\n",
    "    with open(os.path.join(path), \"rb\") as fobj:\n",
    "        return pickle.load(fobj)\n",
    "\n",
    "\n",
    "def try_solve(patience: int, verbose: bool = True):\n",
    "    def decorator(function: Callable[[Any], Any]) -> Callable[[Any], Any]:\n",
    "        def wrapper(*args, **kwargs) -> Any:\n",
    "            for attempt in range(1, patience + 1):\n",
    "                try:\n",
    "                    return function(*args, **kwargs)\n",
    "                except cp.SolverError as err:\n",
    "                    if verbose:\n",
    "                        print(f\"Failed to solve at attempt: {attempt}\")\n",
    "            raise err\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def run_agent(agent: Union[MPC, IOController],\n",
    "              plant: Plant,\n",
    "              use_foresight: bool,\n",
    "              disturbance_bias: Optional[np.ndarray] = None,\n",
    "              bias_aware: bool = True,\n",
    "              rng: np.random.Generator = None,\n",
    "              ) -> List[Transition]:\n",
    "    \"\"\" Simulate the agent in the Fighter environment for 1 trajectory\n",
    "\n",
    "    Args:\n",
    "        agent (MPC): MPC or IO controller\n",
    "        env_length (int): Length of the environment\n",
    "        use_foresight (bool): If true, feed the agent with future noise signal\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    state_disturbance = plant.state_disturbance.copy()\n",
    "    if disturbance_bias is not None and not bias_aware:\n",
    "        state_disturbance -= disturbance_bias\n",
    "    evaluator = ControlLoop(\n",
    "        state_disturbance=state_disturbance,\n",
    "        output_disturbance=plant.output_disturbance,\n",
    "        plant=plant,\n",
    "        controller=agent,\n",
    "        rng=np.random.default_rng(rng.integers(0, 2**30))\n",
    "    )\n",
    "    return evaluator.simulate(\n",
    "        initial_state=None,\n",
    "        use_foresight=use_foresight,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_mpc(env_length: int = 60,\n",
    "            horizon: int = 20,\n",
    "            use_foresight: bool = True,\n",
    "            disturbance_bias: Optional[np.ndarray] = None,\n",
    "            bias_aware: bool = True,\n",
    "            rng: np.random.Generator = None,\n",
    "            ) -> List[Transition]:\n",
    "    \"\"\" Run MPC agent\n",
    "\n",
    "    Args:\n",
    "        horizon (int, optional): Noise horizon of MPC. Defaults to 20.\n",
    "        env_length (int): Length of the environment\n",
    "        use_foresight (bool): If true, feed the agent with future noise signal\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    plant = FighterEnv(\n",
    "        max_length=env_length,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        rng=np.random.default_rng(rng.integers(0, 2**30)))\n",
    "    plant = LinearizationWrapper(plant)\n",
    "    agent = MPC(\n",
    "        action_size=plant.action_size,\n",
    "        state_size=plant.state_size,\n",
    "        noise_size=plant.noise_size,\n",
    "        output_size=plant.output_size,\n",
    "        horizon=horizon)\n",
    "    agent.optimizer = agent.prepare_optimizer(\n",
    "        plant.nominal_model(\n",
    "            lin_point=InputValues(\n",
    "                state=np.zeros((6,)),\n",
    "                action=np.zeros((2,)),\n",
    "                noise=np.zeros((2,)),\n",
    "            )\n",
    "        ))\n",
    "    return run_agent(\n",
    "        agent=agent,\n",
    "        plant=plant,\n",
    "        use_foresight=use_foresight,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        bias_aware=bias_aware,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_rmpc(env_length: int = 60,\n",
    "             horizon: int = 20,\n",
    "             use_foresight: bool = True,\n",
    "             rho: float = 0.1,\n",
    "             disturbance_bias: Optional[np.ndarray] = None,\n",
    "             bias_aware: bool = True,\n",
    "             rng: np.random.Generator = None,\n",
    "             ) -> List[Transition]:\n",
    "    \"\"\" Run Robust MPC\n",
    "\n",
    "    Args:\n",
    "        horizon (int, optional): Noise horizon of MPC. Defaults to 20.\n",
    "        rho (float, optional): Robustness radius. Defaults to 0.1.\n",
    "        env_length (int): Length of the environment\n",
    "        use_foresight (bool): If true, feed the agent with future noise signal\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    plant = FighterEnv(\n",
    "        max_length=env_length,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        rng=np.random.default_rng(rng.integers(0, 2**30)))\n",
    "    plant = LinearizationWrapper(plant)\n",
    "    agent = RobustMPC(action_size=plant.action_size,\n",
    "                      state_size=plant.state_size,\n",
    "                      noise_size=plant.noise_size,\n",
    "                      output_size=plant.output_size,\n",
    "                      horizon=horizon,\n",
    "                      rho=rho,\n",
    "                      state_constraints_flag=True,\n",
    "                      input_constraints_flag=True)\n",
    "    agent.optimizer = agent.prepare_optimizer(\n",
    "        plant.nominal_model(\n",
    "            lin_point=InputValues(\n",
    "                state=np.zeros((6,)),\n",
    "                action=np.zeros((2,)),\n",
    "                noise=np.zeros((2,)),\n",
    "            )\n",
    "        ))\n",
    "    return run_agent(\n",
    "        agent=agent,\n",
    "        plant=plant,\n",
    "        use_foresight=use_foresight,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        bias_aware=bias_aware,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_io(dataset: List[Transition],\n",
    "               rng: np.random.Generator,\n",
    "               expert_class: Union[Type[MPC], Type[RobustMPC]],\n",
    "               expert_kwargs: Dict[str, Any],\n",
    "               env_length: int = 60,\n",
    "               n_past: int = 1,\n",
    "               disturbance_bias: Optional[np.ndarray] = None,\n",
    "               ) -> Tuple[Union[\n",
    "                   Plant,\n",
    "                   FeatureHandler,\n",
    "                   List[AugmentedTransition]]]:\n",
    "    plant = FighterEnv(\n",
    "        max_length=env_length,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        rng=np.random.default_rng(rng.integers(0, 2**30)))\n",
    "    plant = LinearizationWrapper(plant)\n",
    "    nominal_model = plant.nominal_model(\n",
    "            lin_point=InputValues(\n",
    "                state=np.zeros((6,)),\n",
    "                action=np.zeros((2,)),\n",
    "                noise=np.zeros((2,)),\n",
    "            )\n",
    "        )\n",
    "    feature_handler = FeatureHandler(\n",
    "        params=nominal_model,\n",
    "        n_past=n_past,\n",
    "        add_bias=True,\n",
    "        use_action_regressor=False,\n",
    "        use_noise_regressor=True,\n",
    "        use_state_regressor=False)\n",
    "    expert_agent = expert_class(\n",
    "        action_size=plant.action_size,\n",
    "        state_size=plant.state_size,\n",
    "        noise_size=plant.noise_size,\n",
    "        output_size=plant.output_size,\n",
    "        **expert_kwargs)\n",
    "    expert_agent.optimizer = expert_agent.prepare_optimizer(nominal_model)\n",
    "    augmenter = AugmentDataset(\n",
    "        expert_agent=expert_agent,\n",
    "        feature_handler=feature_handler\n",
    "    )\n",
    "    augmented_dataset = augmenter(dataset)\n",
    "    return plant, feature_handler, augmented_dataset\n",
    "\n",
    "\n",
    "@try_solve(patience=2)\n",
    "def run_io(plant: Plant,\n",
    "           feature_handler: FeatureHandler,\n",
    "           augmented_dataset: List[AugmentedTransition],\n",
    "           rng: np.random.Generator,\n",
    "           dataset_length: int = 300,\n",
    "           disturbance_bias: Optional[np.ndarray] = None,\n",
    "           bias_aware: bool = False,\n",
    "           ) -> Callable[[Any], Any]:\n",
    "\n",
    "    io_agent = IOController(\n",
    "        params=feature_handler.params,\n",
    "        include_constraints=True,\n",
    "        soften_state_constraints=True,\n",
    "        state_constraints_flag=True,\n",
    "        action_constraints_flag=True,\n",
    "        dataset_length=dataset_length,\n",
    "        feature_handler=feature_handler)\n",
    "    io_agent.train(\n",
    "        augmented_dataset,\n",
    "        rng=np.random.default_rng(rng.integers(0, 2**30)))\n",
    "    io_agent.action_optimizer = io_agent.prepare_action_optimizer()\n",
    "    return partial(run_agent,\n",
    "                   plant=plant,\n",
    "                   agent=io_agent,\n",
    "                   disturbance_bias=disturbance_bias,\n",
    "                   bias_aware=bias_aware,\n",
    "                   use_foresight=False,   # IO agent does not look into the future\n",
    "                   )\n",
    "\n",
    "\n",
    "def run_io_mpc(dataset: List[Transition],\n",
    "               rng: np.random.Generator,\n",
    "               env_length: int = 60,\n",
    "               n_past: int = 1,\n",
    "               dataset_length: int = 300,\n",
    "               disturbance_bias: Optional[np.ndarray] = None,\n",
    "               bias_aware: bool = True,\n",
    "               expert_horizon: int = 20,\n",
    "               ) -> Callable[[Any], Any]:\n",
    "    \"\"\" Train and simulate IO agent with MPC as the expert\n",
    "\n",
    "    Args:\n",
    "        dataset (List[Transition]): List if transitions to be used\n",
    "            as the training data\n",
    "        horizon (int, optional): Horizon of the expert agent. Defaults to 20.\n",
    "        env_length (int, optional): Length of the environment. Defaults to 60.\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    (plant,\n",
    "     feature_handler,\n",
    "     augmented_dataset\n",
    "     ) = prepare_io(\n",
    "        dataset=dataset,\n",
    "        rng=rng,\n",
    "        expert_class=MPC,\n",
    "        expert_kwargs={\"horizon\": expert_horizon},\n",
    "        env_length=env_length,\n",
    "        n_past=n_past,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "    )\n",
    "    return run_io(\n",
    "        plant=plant,\n",
    "        feature_handler=feature_handler,\n",
    "        augmented_dataset=augmented_dataset,\n",
    "        rng=rng,\n",
    "        dataset_length=dataset_length,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        bias_aware=bias_aware,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_io_rmpc(dataset: List[Transition],\n",
    "                rng: np.random.Generator,\n",
    "                expert_rho: float,\n",
    "                n_past: int = 1,\n",
    "                env_length: int = 60,\n",
    "                dataset_length: int = 300,\n",
    "                disturbance_bias: Optional[np.ndarray] = None,\n",
    "                bias_aware: bool = True,\n",
    "                expert_horizon: int = 20,\n",
    "                ) -> Callable[[Any], Any]:\n",
    "    \"\"\" Train and simulate IO agent with Robust MPC as the expert\n",
    "\n",
    "    Args:\n",
    "        dataset (List[Transition]): List if transitions to be used\n",
    "            as the training data\n",
    "        horizon (int, optional): Horizon of the expert agent. Defaults to 20.\n",
    "        env_length (int, optional): Length of the environment. Defaults to 60.\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    (plant,\n",
    "     feature_handler,\n",
    "     augmented_dataset\n",
    "     ) = prepare_io(\n",
    "        dataset=dataset,\n",
    "        rng=rng,\n",
    "        expert_class=RobustMPC,\n",
    "        expert_kwargs={\n",
    "            \"horizon\": expert_horizon,\n",
    "            \"rho\": expert_rho,\n",
    "            \"state_constraints_flag\": True,\n",
    "            \"input_constraints_flag\": True\n",
    "        },\n",
    "        env_length=env_length,\n",
    "        n_past=n_past,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "    )\n",
    "    return run_io(\n",
    "        plant=plant,\n",
    "        feature_handler=feature_handler,\n",
    "        augmented_dataset=augmented_dataset,\n",
    "        rng=rng,\n",
    "        dataset_length=dataset_length,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        bias_aware=bias_aware,\n",
    "    )\n",
    "\n",
    "\n",
    "def make_figure(cost_data: Dict[str, List[float]],\n",
    "                title: str,\n",
    "                color_list: List[str] = px.colors.qualitative.T10\n",
    "                ) -> go.FigureWidget:\n",
    "    \"\"\" Create a cost density plot\n",
    "\n",
    "    Args:\n",
    "        cost_data (Dict[str, List[float]]): Mapping of agents to cost list\n",
    "        title (str): Title of the plot\n",
    "        color_list (List[str], optional): Color list. Defaults to px.colors.qualitative.T10.\n",
    "\n",
    "    Returns:\n",
    "        go.FigureWidget: Plot widget\n",
    "    \"\"\"\n",
    "    cost_label_pair = list(cost_data.items())\n",
    "    costs = [item[1] for item in cost_label_pair]\n",
    "    labels = [item[0] for item in cost_label_pair]\n",
    "    colors = [color_list[index % len(color_list)] for index in range(len(labels))]\n",
    "\n",
    "    fig = ff.create_distplot(\n",
    "        costs,\n",
    "        group_labels=labels,\n",
    "        colors=colors,\n",
    "        bin_size=4,\n",
    "        show_rug=False)\n",
    "    for color, cost_list in zip(colors, costs):\n",
    "        fig.add_vline(\n",
    "            x=np.median(cost_list),\n",
    "            line_width=3,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=color\n",
    "        )\n",
    "\n",
    "    common_axis_layout = dict(\n",
    "        showline=True,\n",
    "        linecolor=\"#a2a2a2\",\n",
    "        linewidth=1,\n",
    "        showgrid=True,\n",
    "        gridcolor=\"#a2a2a2\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title=dict(text=f\"{title}\", x=0.5),\n",
    "        yaxis=dict(\n",
    "            **common_axis_layout,\n",
    "            title=dict(text=\"density\"),\n",
    "            #  type=\"log\"\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            **common_axis_layout,\n",
    "            title=dict(text=\"cost\")\n",
    "        ),\n",
    "        bargap=0.1,\n",
    "        font=dict(\n",
    "            size=12,\n",
    "            color=\"Black\"\n",
    "        )\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Cost Distributions\n",
    "\n",
    "- Experiment in Figure 1 Left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proc = 20  # Choose this based on your CPU\n",
    "n_trials = 20\n",
    "env_length = 51\n",
    "horizon = 20\n",
    "n_past = 1\n",
    "seed_rng = np.random.default_rng(42)\n",
    "\n",
    "# run_mpc(use_foresight=False,  # Without hindsight\n",
    "#         horizon=horizon,\n",
    "#         env_length=env_length,\n",
    "#         rng=np.random.default_rng(seed_rng))\n",
    "\n",
    "mpc_obl_trajectories = parallelize(\n",
    "    n_proc, run_mpc, [dict(use_foresight=False,  # Without hindsight\n",
    "                           horizon=horizon,\n",
    "                           env_length=env_length,\n",
    "                           rng=np.random.default_rng(_seed))\n",
    "                      for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "    loading_bar_kwargs={\"desc\": \"MPC-obl trials\"})\n",
    "mpc_dst_trajectories = parallelize(\n",
    "    n_proc, run_mpc, [dict(use_foresight=True,  # With hindsight\n",
    "                           horizon=horizon,\n",
    "                           env_length=env_length,\n",
    "                           rng=np.random.default_rng(_seed))\n",
    "                      for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "    loading_bar_kwargs={\"desc\": \"MPC-dst trials\"})\n",
    "\n",
    "io_mpc_dst_trajectories = parallelize(\n",
    "        n_proc, run_io_mpc(mpc_obl_trajectories[:10],\n",
    "                            dataset_length=200,\n",
    "                            n_past=n_past,\n",
    "                            rng=np.random.default_rng(seed_rng.integers(0, 2**30))),\n",
    "        [dict(rng=np.random.default_rng(_seed))\n",
    "            for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "            loading_bar_kwargs={\"desc\": \"IO-MPC trials\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cost distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_obl_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_obl_trajectories])]\n",
    "mpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_dst_trajectories])]\n",
    "io_mpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_mpc_dst_trajectories])]\n",
    "\n",
    "fig = make_figure({\n",
    "    \"MPC (obl)\": mpc_obl_costs,\n",
    "    \"MPC (dst)\": mpc_dst_costs,\n",
    "    \"IO-MPC\": io_mpc_dst_costs},\n",
    "    title=f\"Figure 1 left with {len(mpc_obl_trajectories)} trials\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 1 Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 50\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "rho=0.01\n",
    "seed_rng = np.random.default_rng(42)\n",
    "\n",
    "\n",
    "io_mpc_dst_trajectories = parallelize(\n",
    "        n_proc, run_io_mpc(mpc_obl_trajectories[:10],\n",
    "                            dataset_length=200,\n",
    "                            n_past=n_past,\n",
    "                            disturbance_bias=disturbance_bias,\n",
    "                            rng=np.random.default_rng(seed_rng.integers(0, 2**30))),\n",
    "        [dict(rng=np.random.default_rng(_seed))\n",
    "            for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "            loading_bar_kwargs={\"desc\": \"IO-MPC trials\"})\n",
    "io_rmpc_dst_trajectories = parallelize(\n",
    "        n_proc, run_io_rmpc(mpc_obl_trajectories[:10],\n",
    "                            dataset_length=200,\n",
    "                            n_past=n_past,\n",
    "                            expert_rho=rho,\n",
    "                            disturbance_bias=disturbance_bias,\n",
    "                            rng=np.random.default_rng(seed_rng.integers(0, 2**30))),\n",
    "        [dict(rng=np.random.default_rng(_seed))\n",
    "            for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "            loading_bar_kwargs={\"desc\": \"IO-RMPC trials\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_mpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_mpc_dst_trajectories])]\n",
    "io_rmpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_rmpc_dst_trajectories])]\n",
    "\n",
    "fig = make_figure({\n",
    "    \"IO-MPC\": io_mpc_dst_costs,\n",
    "    \"IO-RMPC\": io_rmpc_dst_costs,\n",
    "    },\n",
    "    title=f\"Figure 1 middle with {len(io_rmpc_dst_trajectories)} trials\",\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 1 Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 50\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "seed_rng = np.random.default_rng(42)\n",
    "\n",
    "mpc_obl_trajectories = parallelize(\n",
    "    n_proc, run_mpc, [dict(use_foresight=False,\n",
    "                           horizon=horizon,\n",
    "                           env_length=env_length,\n",
    "                           disturbance_bias=disturbance_bias,\n",
    "                           rng=np.random.default_rng(_seed))\n",
    "                      for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "                      loading_bar_kwargs={\"desc\": \"MPC-obl trials\"})\n",
    "mpc_fdst_trajectories = parallelize(\n",
    "    n_proc, run_mpc, [dict(use_foresight=True,\n",
    "                           horizon=horizon,\n",
    "                           env_length=env_length,\n",
    "                           disturbance_bias=disturbance_bias,\n",
    "                           bias_aware=True, # Bias in the noise is known\n",
    "                           rng=np.random.default_rng(_seed))\n",
    "                      for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "                      loading_bar_kwargs={\"desc\": \"MPC-fdst trials\"})\n",
    "mpc_pdst_trajectories = parallelize(\n",
    "    n_proc, run_mpc, [dict(use_foresight=True,\n",
    "                           horizon=horizon,\n",
    "                           env_length=env_length,\n",
    "                           disturbance_bias=disturbance_bias,\n",
    "                           bias_aware=False, # Bias is the noise is not known\n",
    "                           rng=np.random.default_rng(_seed))\n",
    "                      for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "                      loading_bar_kwargs={\"desc\": \"MPC-pdst trials\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_obl_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_obl_trajectories])]\n",
    "mpc_fdst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_fdst_trajectories])]\n",
    "mpc_pdst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_pdst_trajectories])]\n",
    "io_rmpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_rmpc_dst_trajectories])]\n",
    "\n",
    "fig = make_figure({\n",
    "        \"MPC (obl)\": mpc_obl_costs,\n",
    "        \"MPC (f-dst)\": mpc_fdst_costs,\n",
    "        \"MPC (p-dst)\": mpc_pdst_costs,\n",
    "        \"IO-RMPC\": io_rmpc_dst_costs,\n",
    "    },\n",
    "    title=f\"Figure 1 Right with {len(io_rmpc_dst_trajectories)} trials\"\n",
    ")\n",
    "# fig.update_layout(width=1200, height=600)\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rho_figure(cost_data: Dict[str, Dict[int, List[float]]],\n",
    "                    title: str,\n",
    "                    color_list: List[str] = px.colors.qualitative.T10,\n",
    "                    percentiles: Tuple[int] = (20, 80),\n",
    "                    ) -> go.FigureWidget:\n",
    "    \"\"\" Make error plot as in Figure 2.a and 2.b\n",
    "\n",
    "    Arg:s\n",
    "        cost_data (Dict[str, Dict[int, List[float]]]): Dictionary of costs per rho\n",
    "        title (str): Title of the plot\n",
    "        color_list (List[str], optional): Color list. Defaults to px.colors.qualitative.T10.\n",
    "        percentiles (Tuple[int]): Lower and Upper percentiles.\n",
    "\n",
    "    Returns:\n",
    "        go.FigureWidget: Plot widget\n",
    "    \"\"\"\n",
    "    fig = go.FigureWidget()\n",
    "    cost_label_pair = list(cost_data.items())\n",
    "    cost_data = [item[1] for item in cost_label_pair]\n",
    "    labels = [item[0] for item in cost_label_pair]\n",
    "    colors = [color_list[index % len(color_list)] for index in range(len(labels))]\n",
    "\n",
    "    percentile_lower, percentile_up = percentiles\n",
    "    for color, cost_dict, label in zip(colors, cost_data, labels):\n",
    "        rho_values = {rho: np.percentile(cost_list, [percentile_lower, 50, percentile_up])\n",
    "                      for rho, cost_list in cost_dict.items()}\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(rho_values.keys()),\n",
    "            y=[item[1] for item in rho_values.values()],\n",
    "            line=dict(color=color),\n",
    "            mode=\"lines\",\n",
    "            name=label,\n",
    "            legendgroup=label\n",
    "        ))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Upper Bound\",\n",
    "                x=list(rho_values.keys()),\n",
    "                y=[item[2] for item in rho_values.values()],\n",
    "                mode=\"lines\",\n",
    "                marker=dict(color=color),\n",
    "                line=dict(width=0),\n",
    "                showlegend=False,\n",
    "                legendgroup=label\n",
    "            ))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Lower Bound\",\n",
    "                x=list(rho_values.keys()),\n",
    "                y=[item[0] for item in rho_values.values()],\n",
    "                marker=dict(color=color),\n",
    "                line=dict(width=0),\n",
    "                mode=\"lines\",\n",
    "                # fillcolor=color,\n",
    "                opacity=0.5,\n",
    "                fill=\"tonexty\",\n",
    "                showlegend=False,\n",
    "                legendgroup=label\n",
    "            ))\n",
    "\n",
    "    common_axis_layout = dict(\n",
    "        showline=True,\n",
    "        linecolor=\"#a2a2a2\",\n",
    "        linewidth=1,\n",
    "        showgrid=True,\n",
    "        gridcolor=\"#a2a2a2\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title=dict(text=f\"{title}\", x=0.5),\n",
    "        yaxis=dict(\n",
    "            **common_axis_layout,\n",
    "            title=dict(text=\"costs\"),\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            **common_axis_layout,\n",
    "            title=dict(text=\"uncertainty radius\"),\n",
    "            type=\"log\"\n",
    "        ),\n",
    "        font=dict(\n",
    "            size=12,\n",
    "            color=\"Black\"\n",
    "        )\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 2 Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_length = 200\n",
    "n_obl_trajectories = 20\n",
    "n_io_agents = 50\n",
    "n_io_trials = 50\n",
    "n_rho = 12\n",
    "env_length = 60\n",
    "expert_horizon = 20\n",
    "n_proc = 9  # Choose this based on your CPU cores\n",
    "seed_rng = np.random.default_rng(42)\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def async_execute_run_io(*args, **kwargs):\n",
    "    return run_io(*args, **kwargs)\n",
    "\n",
    "\n",
    "def run_rho_experiment(rho: float,\n",
    "                       rng: np.random.Generator,\n",
    "                       mpc_obl_trajectories: List[List[Transition]]):\n",
    "    rho_io_rmpc_costs = []\n",
    "    # Augment the obl trajectories(dataset) with the RMPC actions\n",
    "    plant, feature_handler, augmented_dataset = prepare_io(\n",
    "        dataset=mpc_obl_trajectories,\n",
    "        rng=np.random.default_rng(rng.integers(1, 2**30)),\n",
    "        expert_class=RobustMPC,\n",
    "        expert_kwargs={\n",
    "            \"horizon\": expert_horizon,\n",
    "            \"rho\": rho,\n",
    "            \"state_constraints_flag\": True,\n",
    "            \"input_constraints_flag\": True\n",
    "        },\n",
    "        env_length=env_length,\n",
    "        n_past=n_past,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "    )\n",
    "\n",
    "    # Train ```n_io_agents``` many agents\n",
    "    io_agent_runables = parallelize(\n",
    "        n_proc=n_proc,\n",
    "        fn=async_execute_run_io,\n",
    "        kwargs_list=[dict(\n",
    "            plant=plant,\n",
    "            feature_handler=feature_handler,\n",
    "            augmented_dataset=augmented_dataset,\n",
    "            rng=np.random.default_rng(_seed),\n",
    "            dataset_length=train_dataset_length,\n",
    "            disturbance_bias=disturbance_bias,\n",
    "            bias_aware=False,\n",
    "        ) for _seed in rng.integers(0, 2**30, size=n_io_agents)],\n",
    "    )\n",
    "\n",
    "    # Evaluate all io_agents\n",
    "    for io_agent_runner in tqdm(io_agent_runables, desc=f\"Evaluate IO-(rho={np.round(rho, decimals=4)})\"):\n",
    "        io_rmpc_trajectories = parallelize(\n",
    "            n_proc=n_proc,\n",
    "            fn=io_agent_runner,\n",
    "            kwargs_list=[dict(rng=np.random.default_rng(_seed))\n",
    "                         for _seed in rng.integers(0, 2**30, size=n_io_trials)])\n",
    "        costs = [trans.cost for trans in chain(\n",
    "            *[traj[int(60 * 0.6):] for traj in io_rmpc_trajectories])]\n",
    "        rho_io_rmpc_costs.append(np.mean(costs))  # Append the average of the steady state cost\n",
    "    return rho_io_rmpc_costs\n",
    "\n",
    "\n",
    "# Gather ```n_obl_trajectories``` many mpc obl trajectories\n",
    "mpc_obl_trajectories = parallelize(\n",
    "    n_proc=n_proc,\n",
    "    fn=run_mpc,\n",
    "    kwargs_list=[dict(use_foresight=False,\n",
    "                      horizon=horizon,\n",
    "                      env_length=env_length,\n",
    "                      disturbance_bias=disturbance_bias,\n",
    "                      rng=np.random.default_rng(_seed))\n",
    "                 for _seed in seed_rng.integers(0, 2**30, size=n_obl_trajectories)])\n",
    "\n",
    "rho_keys = np.logspace(-3, -1.6, n_rho)\n",
    "rho_values = parallelize(\n",
    "    n_proc=12,\n",
    "    fn=run_rho_experiment,\n",
    "    kwargs_list=[dict(rho=_rho,\n",
    "                      rng=np.random.default_rng(_seed),\n",
    "                      mpc_obl_trajectories=mpc_obl_trajectories)\n",
    "                 for _rho, _seed in zip(rho_keys, seed_rng.integers(0, 2**30, size=len(rho_keys)))],\n",
    "    loading_bar_kwargs=dict(desc=\"rho values\")\n",
    ")\n",
    "io_rmpc_rho_costs = {key: value for key, value in zip(rho_keys, rho_values)}\n",
    "\n",
    "# save_experiment(io_rmpc_rho_costs, seed=42, exp_dir=\"./results\", name=\"figure_2_left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_rho_figure({\n",
    "    \"IO-RMPC\": io_rmpc_rho_costs,\n",
    "    \"MPC (f-dst)\": {rho: mpc_fdst_costs for rho in io_rmpc_rho_costs.keys()}\n",
    "}, title=\"Figure 2 Left\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 2 Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obl_trajectories = 10\n",
    "n_trials = 100\n",
    "seed_rng = np.random.default_rng(42)\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Gather ```n_obl_trajectories``` many mpc obl trajectories\n",
    "mpc_obl_trajectories = parallelize(\n",
    "    n_proc=n_proc,\n",
    "    fn=run_mpc,\n",
    "    kwargs_list=[dict(use_foresight=False,\n",
    "                      horizon=horizon,\n",
    "                      env_length=env_length,\n",
    "                      disturbance_bias=disturbance_bias,\n",
    "                      rng=np.random.default_rng(_seed))\n",
    "                 for _seed in seed_rng.integers(0, 2**30, size=n_obl_trajectories)])\n",
    "\n",
    "\n",
    "io_rmpc_trajectories = {}\n",
    "for rho in np.logspace(-3, -1.6, 12):\n",
    "    io_rmpc_trajectories[rho] = parallelize(\n",
    "        n_proc, run_io_rmpc(mpc_obl_trajectories,\n",
    "                            dataset_length=200,\n",
    "                            n_past=n_past,\n",
    "                            expert_rho=rho,\n",
    "                            disturbance_bias=disturbance_bias,\n",
    "                            rng=np.random.default_rng(seed_rng.integers(0, 2**30))),\n",
    "        [dict(rng=np.random.default_rng(_seed))\n",
    "            for _seed in seed_rng.integers(0, 2**30, size=n_trials)],\n",
    "            loading_bar_kwargs=dict(desc=\"rho values\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_rmpc_costs = {rho: [trans.cost for trans in chain(*[traj[int(60 * 0.6):] for traj in trajectories])]\n",
    "    for rho, trajectories in io_rmpc_trajectories.items()}\n",
    "\n",
    "make_rho_figure(\n",
    "    {\n",
    "        \"IO-RMPC\": io_rmpc_costs,\n",
    "        \"MPC (f-dst)\": {rho: mpc_fdst_costs for rho in io_rmpc_costs.keys()},\n",
    "        \"IO MPC\": {rho: io_mpc_dst_costs for rho in io_rmpc_costs.keys()}\n",
    "        },\n",
    "    title=\"Figure 2 Middle\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 2 Right\n",
    "\n",
    "Optimal IO-RMPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_rho, opt_costs = min(list(io_rmpc_costs.items()), key=lambda item: np.median(item[1]))\n",
    "\n",
    "fig = make_figure({\n",
    "        \"MPC (f-dst)\": mpc_fdst_costs,\n",
    "        f\"IO-RMPC(rho*={np.round(opt_rho, decimals=4)})\": opt_costs,\n",
    "        \"IO-MPC\": io_mpc_dst_costs\n",
    "    },\n",
    "    title=f\"Figure 2 Right with {len(io_rmpc_dst_trajectories)} trials\"\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
