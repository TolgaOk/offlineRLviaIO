{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fighter Environment\n",
    "\n",
    "Experiments and figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Union\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "from io_agent.plant.fighter import FighterEnv, fighter_env_params\n",
    "from io_agent.trainer import ControlLoop, Transition\n",
    "from io_agent.control.mpc import MPC\n",
    "from io_agent.control.rmpc import RobustMPC\n",
    "from io_agent.control.io import IOController\n",
    "from io_agent.utils import FeatureHandler\n",
    "\n",
    "\n",
    "def run_agent(agent: Union[MPC, IOController],\n",
    "              env_length: int,\n",
    "              use_foresight: bool,\n",
    "              disturbance_bias: Optional[np.ndarray] = None,\n",
    "              bias_aware: bool = True,\n",
    "              ) -> List[Transition]:\n",
    "    \"\"\" Simulate the agent in the Fighter environment for 1 trajectory\n",
    "\n",
    "    Args:\n",
    "        agent (MPC): MPC or IO controller\n",
    "        env_length (int): Length of the environment\n",
    "        use_foresight (bool): If true, feed the agent with future noise signal\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    plant = FighterEnv(max_length=env_length, env_params=fighter_env_params, disturbance_bias=disturbance_bias)\n",
    "    state_disturbance = plant.state_disturbance.copy()\n",
    "    if disturbance_bias is not None and not bias_aware:\n",
    "        state_disturbance -= disturbance_bias\n",
    "    trainer = ControlLoop(\n",
    "        state_disturbance=state_disturbance,\n",
    "        output_disturbance=plant.output_disturbance,\n",
    "        plant=plant,\n",
    "        controller=agent\n",
    "    )\n",
    "    return trainer.simulate(\n",
    "        initial_state=None,\n",
    "        use_foresight=use_foresight,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_mpc(env_length: int = 60,\n",
    "            horizon: int = 20,\n",
    "            use_foresight: bool = True,\n",
    "            disturbance_bias: Optional[np.ndarray] = None,\n",
    "              bias_aware: bool = True,\n",
    "            ) -> List[Transition]:\n",
    "    \"\"\" Run MPC agent\n",
    "\n",
    "    Args:\n",
    "        horizon (int, optional): Noise horizon of MPC. Defaults to 20.\n",
    "        env_length (int): Length of the environment\n",
    "        use_foresight (bool): If true, feed the agent with future noise signal\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    agent = MPC(\n",
    "        env_params=fighter_env_params,\n",
    "        horizon=horizon)\n",
    "    return run_agent(\n",
    "        agent=agent,\n",
    "        env_length=env_length,\n",
    "        use_foresight=use_foresight,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        bias_aware=bias_aware\n",
    "    )\n",
    "\n",
    "\n",
    "def run_rmpc(env_length: int = 60,\n",
    "             horizon: int = 20,\n",
    "             use_foresight: bool = True,\n",
    "             rho: float = 0.1,\n",
    "             disturbance_bias: Optional[np.ndarray] = None,\n",
    "              bias_aware: bool = True,\n",
    "             ) -> List[Transition]:\n",
    "    \"\"\" Run Robust MPC\n",
    "\n",
    "    Args:\n",
    "        horizon (int, optional): Noise horizon of MPC. Defaults to 20.\n",
    "        rho (float, optional): Robustness radius. Defaults to 0.1.\n",
    "        env_length (int): Length of the environment\n",
    "        use_foresight (bool): If true, feed the agent with future noise signal\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    agent = RobustMPC(env_params=fighter_env_params,\n",
    "                      horizon=horizon,\n",
    "                      rho=rho,\n",
    "                      state_constraints_flag=True,\n",
    "                      input_constraints_flag=True)\n",
    "    return run_agent(\n",
    "        agent=agent,\n",
    "        env_length=env_length,\n",
    "        use_foresight=use_foresight,\n",
    "        disturbance_bias=disturbance_bias,\n",
    "        bias_aware=bias_aware\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_io(dataset: List[Transition],\n",
    "           expert_agent: MPC,\n",
    "           expert_horizon: int = 20,\n",
    "           env_length: int = 60,\n",
    "           ) -> IOController:\n",
    "    \"\"\" Make and train an IO agent\n",
    "\n",
    "    Args:\n",
    "        dataset (List[Transition]): List if transitions to be used\n",
    "            as the training data\n",
    "        expert_agent (MPC): Expert agent to compute expert actions in the\n",
    "            training data\n",
    "        expert_horizon (int, optional): Horizon of the expert agent. Defaults to 20.\n",
    "        env_length (int, optional): Length of the environment. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        IOController: Trained IO agent\n",
    "    \"\"\"\n",
    "    feature_handler = FeatureHandler(\n",
    "        env_params=fighter_env_params,\n",
    "        n_accumulate=expert_horizon + 1,\n",
    "        n_past=1,\n",
    "        add_bias=True,\n",
    "        use_action_regressor=False,\n",
    "        use_noise_regressor=True,\n",
    "        use_state_regressor=False)\n",
    "    agent = IOController(\n",
    "        env_params=fighter_env_params,\n",
    "        expert_agent=expert_agent,\n",
    "        include_constraints=True,\n",
    "        soften_state_constraints=True,\n",
    "        state_constraints_flag=True,\n",
    "        action_constraints_flag=True,\n",
    "        dataset_length=(env_length - feature_handler.n_past) * len(dataset),\n",
    "        feature_handler=feature_handler)\n",
    "    agent.train(dataset)\n",
    "    agent.action_optimizer = agent.prepare_action_optimizer()\n",
    "    return agent\n",
    "    \n",
    "\n",
    "def run_io_mpc(dataset: List[Transition],\n",
    "               n_trials: int = 10,\n",
    "             horizon: int = 20,\n",
    "             env_length: int = 60,\n",
    "             disturbance_bias: Optional[np.ndarray] = None,\n",
    "              bias_aware: bool = True,\n",
    "             ) -> List[Transition]:\n",
    "    \"\"\" Train and simulate IO agent with MPC as the expert\n",
    "\n",
    "    Args:\n",
    "        dataset (List[Transition]): List if transitions to be used\n",
    "            as the training data\n",
    "        n_trials (int, optional): Number of trajectories to run IO agent. Defaults to 10.\n",
    "        horizon (int, optional): Horizon of the expert agent. Defaults to 20.\n",
    "        env_length (int, optional): Length of the environment. Defaults to 60.\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    expert_agent = MPC(\n",
    "        env_params=fighter_env_params,\n",
    "        horizon=horizon)\n",
    "    io_agent = prepare_io(dataset,\n",
    "           expert_agent=expert_agent,\n",
    "           env_length=env_length)\n",
    "    return [run_agent(\n",
    "            agent=io_agent,\n",
    "            env_length=env_length,\n",
    "            disturbance_bias=disturbance_bias,\n",
    "            bias_aware=bias_aware,\n",
    "            use_foresight=False,   # IO agent does not look into the future\n",
    "        ) for _ in tqdm(range(n_trials))]\n",
    "\n",
    "def run_io_rmpc(dataset: List[Transition],\n",
    "             rho: float,  \n",
    "             n_trials: int = 10,\n",
    "             horizon: int = 20,\n",
    "             env_length: int = 60,\n",
    "             disturbance_bias: Optional[np.ndarray] = None,\n",
    "              bias_aware: bool = True,\n",
    "             ) -> List[Transition]:\n",
    "    \"\"\" Train and simulate IO agent with Robust MPC as the expert\n",
    "\n",
    "    Args:\n",
    "        dataset (List[Transition]): List if transitions to be used\n",
    "            as the training data\n",
    "        n_trials (int, optional): Number of trajectories to run IO agent. Defaults to 10.\n",
    "        horizon (int, optional): Horizon of the expert agent. Defaults to 20.\n",
    "        env_length (int, optional): Length of the environment. Defaults to 60.\n",
    "        disturbance_bias (Optional[np.ndarray], optional): Bias for the state disturbance. Defaults to 0.\n",
    "        bias_aware (bool, optional): _description_. If true, feed the agent with actual noise (biased).\n",
    "\n",
    "    Returns:\n",
    "        List[Transition]: Trajectory of transitions\n",
    "    \"\"\"\n",
    "    expert_agent = RobustMPC(env_params=fighter_env_params,\n",
    "                    horizon=horizon,\n",
    "                    rho=rho,\n",
    "                    state_constraints_flag=True,\n",
    "                    input_constraints_flag=True)\n",
    "    io_agent = prepare_io(dataset,\n",
    "           expert_agent=expert_agent,\n",
    "           env_length=env_length)\n",
    "    return [run_agent(\n",
    "            agent=io_agent,\n",
    "            env_length=env_length,\n",
    "            disturbance_bias=disturbance_bias,\n",
    "            bias_aware=bias_aware,\n",
    "            use_foresight=False,   # IO agent does not look into the future\n",
    "        ) for _ in tqdm(range(n_trials))]\n",
    "\n",
    "\n",
    "def make_figure(cost_data: Dict[str, List[float]],\n",
    "                title: str,\n",
    "                color_list: List[str] = px.colors.qualitative.T10\n",
    "    ) -> go.FigureWidget:\n",
    "    \"\"\" Create a cost density plot\n",
    "\n",
    "    Args:\n",
    "        cost_data (Dict[str, List[float]]): Mapping of agents to cost list\n",
    "        title (str): Title of the plot\n",
    "        color_list (List[str], optional): Color list. Defaults to px.colors.qualitative.T10.\n",
    "\n",
    "    Returns:\n",
    "        go.FigureWidget: Plot widget\n",
    "    \"\"\"\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(\n",
    "    )\n",
    "    \n",
    "    cost_label_pair = list(cost_data.items())\n",
    "    costs = [item[1] for item in cost_label_pair]\n",
    "    labels = [item[0] for item in cost_label_pair]\n",
    "    colors = [color_list[index % len(color_list)] for index in range(len(labels))]\n",
    "\n",
    "    fig = ff.create_distplot(\n",
    "        costs,\n",
    "        group_labels=labels,\n",
    "        colors=colors,\n",
    "        bin_size=4,\n",
    "        show_rug=False)\n",
    "    for color, cost_list in zip(colors, costs):\n",
    "        fig.add_vline(\n",
    "            x=np.median(cost_list),\n",
    "            line_width=3,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=color\n",
    "            )\n",
    "\n",
    "    common_axis_layout = dict(\n",
    "            showline=True,\n",
    "            linecolor = \"#a2a2a2\",\n",
    "            linewidth = 1,\n",
    "            showgrid = True,\n",
    "            gridcolor = \"#a2a2a2\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title=dict(text=f\"{title}\", x=0.5),\n",
    "        yaxis=dict(\n",
    "            **common_axis_layout,\n",
    "             title=dict(text=\"density\"),\n",
    "            #  type=\"log\"\n",
    "             ),\n",
    "        xaxis=dict(\n",
    "            **common_axis_layout,\n",
    "             title=dict(text=\"cost\")\n",
    "             ),\n",
    "        bargap=0.1,\n",
    "        font=dict(\n",
    "            size=12,\n",
    "            color=\"Black\"\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Cost Distributions\n",
    "\n",
    "- Experiment in Figure 1 Left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "\n",
    "mpc_obl_trajectories = [run_mpc(use_foresight=False) for _ in tqdm(range(n_trials))]\n",
    "mpc_dst_trajectories = [run_mpc(use_foresight=True) for _ in tqdm(range(n_trials))]\n",
    "io_mpc_dst_trajectories = run_io_mpc(mpc_obl_trajectories[:5], n_trials=n_trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cost distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_obl_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_obl_trajectories])]\n",
    "mpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_dst_trajectories])]\n",
    "io_mpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_mpc_dst_trajectories])]\n",
    "\n",
    "fig = make_figure({\n",
    "    \"MPC (obl)\": mpc_obl_costs,\n",
    "    \"MPC (dst)\": mpc_dst_costs,\n",
    "    \"IO-MPC (dst)\": io_mpc_dst_costs,\n",
    "    },\n",
    "    title=f\"Figure 1 left with {len(mpc_dst_trajectories)} trials\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 1 Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 50\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "rho=0.01\n",
    "\n",
    "io_mpc_dst_trajectories = run_io_mpc(mpc_obl_trajectories[:5], disturbance_bias=disturbance_bias, n_trials=n_trials)\n",
    "io_rmpc_dst_trajectories = run_io_rmpc(mpc_obl_trajectories[:5], disturbance_bias=disturbance_bias, rho=rho, n_trials=n_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_mpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_mpc_dst_trajectories])]\n",
    "io_rmpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_rmpc_dst_trajectories])]\n",
    "\n",
    "fig = make_figure({\n",
    "    \"IO-MPC\": io_mpc_dst_costs,\n",
    "    \"IO-RMPC\": io_rmpc_dst_costs,\n",
    "    },\n",
    "    title=f\"Figure 1 middle with {len(io_rmpc_dst_trajectories)} trials\",\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 1 Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 50\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "\n",
    "mpc_obl_trajectories = [run_mpc(use_foresight=False, disturbance_bias=disturbance_bias, bias_aware=True) for _ in tqdm(range(n_trials))]\n",
    "mpc_fdst_trajectories = [run_mpc(use_foresight=True, disturbance_bias=disturbance_bias, bias_aware=True) for _ in tqdm(range(n_trials))]\n",
    "mpc_pdst_trajectories = [run_mpc(use_foresight=True, disturbance_bias=disturbance_bias, bias_aware=False) for _ in tqdm(range(n_trials))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_obl_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_obl_trajectories])]\n",
    "mpc_fdst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_fdst_trajectories])]\n",
    "mpc_pdst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in mpc_pdst_trajectories])]\n",
    "io_rmpc_dst_costs = [transition.cost for transition in chain(*[traj[int(60 * 0.6):] for traj in io_rmpc_dst_trajectories])]\n",
    "\n",
    "fig = make_figure({\n",
    "        \"MPC (obl)\": mpc_obl_costs,\n",
    "        \"MPC (f-dst)\": mpc_fdst_costs,\n",
    "        \"MPC (p-dst)\": mpc_pdst_costs,\n",
    "        \"IO-RMPC\": io_rmpc_dst_costs,\n",
    "    },\n",
    "    title=f\"Figure 1 Right with {len(io_rmpc_dst_trajectories)} trials\"\n",
    ")\n",
    "# fig.update_layout(width=1200, height=600)\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Radius\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rho_figure(cost_data: Dict[str, Dict[int, List[float]]],\n",
    "                    title: str,\n",
    "                    color_list: List[str] = px.colors.qualitative.T10\n",
    "                    ) -> go.FigureWidget:\n",
    "    \"\"\" Make error plot as in Figure 2.a and 2.b\n",
    "\n",
    "    Args:\n",
    "        cost_data (Dict[str, Dict[int, List[float]]]): Dictionary of costs per rho\n",
    "        title (str): Title of the plot\n",
    "        color_list (List[str], optional): Color list. Defaults to px.colors.qualitative.T10.\n",
    "\n",
    "    Returns:\n",
    "        go.FigureWidget: Plot widget\n",
    "    \"\"\"\n",
    "    fig = go.FigureWidget()\n",
    "    cost_label_pair = list(cost_data.items())\n",
    "    cost_data = [item[1] for item in cost_label_pair]\n",
    "    labels = [item[0] for item in cost_label_pair]\n",
    "    colors = [color_list[index % len(color_list)] for index in range(len(labels))]\n",
    "\n",
    "    for color, cost_dict, label in zip(colors, cost_data, labels):\n",
    "        rho_values = {rho: np.percentile(cost_list, [20, 50, 80]) for rho, cost_list in cost_dict.items()}\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(rho_values.keys()),\n",
    "            y=[item[1] for item in rho_values.values()],\n",
    "            line=dict(color=color),\n",
    "            mode=\"lines\",\n",
    "            name=label,\n",
    "            legendgroup=label\n",
    "        ))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "            name=\"Upper Bound\",\n",
    "            x=list(rho_values.keys()),\n",
    "            y=[item[2] for item in rho_values.values()],\n",
    "            mode=\"lines\",\n",
    "            marker=dict(color=color),\n",
    "            line=dict(width=0),\n",
    "            showlegend=False,\n",
    "            legendgroup=label\n",
    "        ))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Lower Bound\",\n",
    "                x=list(rho_values.keys()),\n",
    "                y=[item[0] for item in rho_values.values()],\n",
    "                marker=dict(color=color),\n",
    "                line=dict(width=0),\n",
    "                mode=\"lines\",\n",
    "                # fillcolor=color,\n",
    "                opacity=0.5,\n",
    "                fill=\"tonexty\",\n",
    "                showlegend=False,\n",
    "                legendgroup=label\n",
    "            ))\n",
    "\n",
    "    common_axis_layout = dict(\n",
    "        showline=True,\n",
    "        linecolor=\"#a2a2a2\",\n",
    "        linewidth=1,\n",
    "        showgrid=True,\n",
    "        gridcolor=\"#a2a2a2\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title=dict(text=f\"{title}\", x=0.5),\n",
    "        yaxis=dict(\n",
    "            **common_axis_layout,\n",
    "            title=dict(text=\"costs\"),\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            **common_axis_layout,\n",
    "            title=dict(text=\"uncertainty radius\"),\n",
    "            type=\"log\"\n",
    "        ),\n",
    "        font=dict(\n",
    "            size=12,\n",
    "            color=\"Black\"\n",
    "        )\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 2 Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "n_train_dataset = 1\n",
    "n_datasets = 1\n",
    "n_trials = 2\n",
    "n_train = 3\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "\n",
    "# mpc obl\n",
    "mpc_train_trajectories = [run_mpc(use_foresight=False, bias_aware=True) for _ in tqdm(range(n_datasets))]\n",
    "\n",
    "io_rmpc_rho_costs = defaultdict(list)\n",
    "for rho in np.logspace(-3, -1.6, 2):\n",
    "    print(\"Experiments started with rho:\", rho)\n",
    "    for _ in range(n_train):\n",
    "        sample_dataset_indices = np.random.permutation(n_datasets)[:n_train_dataset]\n",
    "        sample_dataset = [mpc_train_trajectories[index] for index in sample_dataset_indices]\n",
    "        trajectories = run_io_rmpc(dataset=sample_dataset, rho=rho,  disturbance_bias=disturbance_bias, n_trials=n_trials)\n",
    "        \n",
    "        costs = [trans.cost for trans in chain(*[traj[int(60 * 0.6):] for traj in trajectories])]\n",
    "        io_rmpc_rho_costs[rho].append(np.mean(costs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_rho_figure({\n",
    "    \"IO-RMPC\": io_rmpc_rho_costs,\n",
    "    \"MPC (f-dst)\": {rho: mpc_fdst_costs for rho in io_rmpc_rho_costs.keys()}\n",
    "    }, title=\"Figure 2 Left\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 2 Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trajectories = 5\n",
    "n_trials = 50\n",
    "disturbance_bias = np.array([0.0, 0.005]).reshape(-1, 1)\n",
    "\n",
    "# mpc obl\n",
    "mpc_train_trajectories = [run_mpc(use_foresight=False) for _ in tqdm(range(n_trajectories))]\n",
    "\n",
    "io_rmpc_trajectories = {}\n",
    "for rho in np.logspace(-3, -1.6, 12):\n",
    "    io_rmpc_trajectories[rho] = run_io_rmpc(dataset=mpc_train_trajectories, disturbance_bias=disturbance_bias, rho=rho, n_trials=n_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_rmpc_costs = {rho: [trans.cost for trans in chain(*[traj[int(60 * 0.6):] for traj in trajectories])]\n",
    "    for rho, trajectories in io_rmpc_trajectories.items()}\n",
    "\n",
    "make_rho_figure(\n",
    "    {\n",
    "        \"IO-RMPC\": io_rmpc_costs,\n",
    "        \"MPC (f-dst)\": {rho: mpc_fdst_costs for rho in io_rmpc_costs.keys()},\n",
    "        \"IO MPC\": {rho: io_mpc_dst_costs for rho in io_rmpc_costs.keys()}\n",
    "        },\n",
    "    title=\"Figure 2 Middle\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment in Figure 2 Right\n",
    "\n",
    "Optimal IO-RMPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_rho, opt_costs = min(list(io_rmpc_costs.items()), key=lambda item: np.percentile(item[1], 50))\n",
    "\n",
    "fig = make_figure({\n",
    "        \"MPC (f-dst)\": mpc_fdst_costs,\n",
    "        \"IO-RMPC\": io_rmpc_dst_costs,\n",
    "        \"IO-MPC\": io_mpc_dst_costs\n",
    "    },\n",
    "    title=f\"Figure 2 Right with {len(io_rmpc_dst_trajectories)} trials\"\n",
    ")\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
